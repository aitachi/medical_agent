# -*- coding: utf-8 -*-
"""
LLMæœåŠ¡ - æ¥å…¥é˜¿é‡Œäº‘DashScope qwen-plusæ¨¡å‹
"""

import asyncio
import aiohttp
import json
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime

logger = logging.getLogger(__name__)


class DashScopeLLM:
    """
    é˜¿é‡Œäº‘DashScope LLMå®¢æˆ·ç«¯
    æ”¯æŒqwen-plusç­‰æ¨¡å‹
    """

    def __init__(
        self,
        api_key: str,
        base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
        model: str = "qwen-plus",
        timeout: int = 30
    ):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.timeout = timeout
        self.session: Optional[aiohttp.ClientSession] = None

    async def start(self):
        """å¯åŠ¨å®¢æˆ·ç«¯"""
        if self.session is None:
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            self.session = aiohttp.ClientSession(timeout=timeout)
            logger.info(f"[LLM] DashScope client started with model: {self.model}")

    async def stop(self):
        """åœæ­¢å®¢æˆ·ç«¯"""
        if self.session:
            await self.session.close()
            self.session = None
            logger.info("[LLM] DashScope client stopped")

    def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        stream: bool = False,
        **kwargs
    ):
        """
        å‘èµ·èŠå¤©è¯·æ±‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º

        Returns:
            str æˆ– AsyncIterator: æ¨¡å‹å“åº”
        """
        if not stream:
            # éæµå¼ï¼šè¿”å›coroutine
            return self._chat_non_stream(messages, temperature, max_tokens, **kwargs)
        else:
            # æµå¼ï¼šè¿”å›async generator
            return self._chat_stream_gen(messages, temperature, max_tokens, **kwargs)

    async def _chat_non_stream(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> str:
        """éæµå¼è¯·æ±‚"""
        if not self.session:
            await self.start()

        url = f"{self.base_url}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False,
            **kwargs
        }

        try:
            async with self.session.post(url, headers=headers, json=payload) as response:
                if response.status != 200:
                    error_text = await response.text()
                    logger.error(f"[LLM] API error: {response.status} - {error_text}")
                    raise Exception(f"API error {response.status}: {error_text}")

                result = await response.json()
                content = result["choices"][0]["message"]["content"]
                return content

        except asyncio.TimeoutError:
            logger.error(f"[LLM] Request timeout")
            raise Exception("LLM request timeout")
        except Exception as e:
            logger.error(f"[LLM] Request failed: {e}")
            raise

    async def _chat_stream_gen(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ):
        """æµå¼è¯·æ±‚"""
        if not self.session:
            await self.start()

        url = f"{self.base_url}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": True,
            **kwargs
        }

        try:
            async with self.session.post(url, headers=headers, json=payload) as response:
                if response.status != 200:
                    error_text = await response.text()
                    logger.error(f"[LLM] API error: {response.status} - {error_text}")
                    raise Exception(f"API error {response.status}: {error_text}")

                # aiohttp streaming: read line by line
                buffer = ""
                async for chunk in response.content.iter_chunked(1024):
                    if chunk:
                        buffer += chunk.decode('utf-8', errors='ignore')
                        lines = buffer.split('\n')
                        buffer = lines.pop()  # Keep incomplete line in buffer

                        for line in lines:
                            line = line.strip()
                            if line.startswith('data: '):
                                data_str = line[6:]
                                if data_str == '[DONE]':
                                    return
                                try:
                                    data = json.loads(data_str)
                                    if 'choices' in data and len(data['choices']) > 0:
                                        delta = data['choices'][0].get('delta', {})
                                        content = delta.get('content', '')
                                        if content:
                                            yield content
                                except json.JSONDecodeError:
                                    continue

        except asyncio.TimeoutError:
            logger.error(f"[LLM] Request timeout")
            raise Exception("LLM request timeout")
        except Exception as e:
            logger.error(f"[LLM] Stream failed: {e}")
            raise

    async def chat_with_system(
        self,
        user_message: str,
        system_prompt: str,
        conversation_history: List[Dict] = None,
        temperature: float = 0.7
    ) -> str:
        """
        ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯è¿›è¡Œå¯¹è¯

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            conversation_history: å¯¹è¯å†å²
            temperature: æ¸©åº¦å‚æ•°

        Returns:
            str: æ¨¡å‹å“åº”
        """
        messages = [{"role": "system", "content": system_prompt}]

        if conversation_history:
            messages.extend(conversation_history)

        messages.append({"role": "user", "content": user_message})

        return await self.chat(messages, temperature=temperature)


# ============================================================
# åŒ»ç–—ä¸“ç”¨LLMæœåŠ¡
# ============================================================

class MedicalLLMService:
    """
    åŒ»ç–—LLMæœåŠ¡
    ç»“åˆæ„å›¾åˆ†ç±»å’ŒLLMç”Ÿæˆå“åº”
    """

    # åŒ»ç–—åŠ©æ‰‹ç³»ç»Ÿæç¤º
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—å¥åº·åŠ©æ‰‹ï¼Œåä¸º"åŒ»å°åŠ©"ã€‚ä½ çš„èŒè´£æ˜¯ï¼š

## æ ¸å¿ƒåŸåˆ™
1. **å®‰å…¨ç¬¬ä¸€**: å§‹ç»ˆä¼˜å…ˆè€ƒè™‘ç”¨æˆ·å¥åº·å®‰å…¨
2. **ä¸“ä¸šå‡†ç¡®**: æä¾›åŸºäºåŒ»å­¦çŸ¥è¯†çš„å‡†ç¡®ä¿¡æ¯
3. **æ¸©é¦¨å‹å¥½**: ç”¨æ¸©æš–ã€å…³æ€€çš„è¯­æ°”ä¸ç”¨æˆ·äº¤æµ
4. **ä¸è¯Šæ–­**: æ˜ç¡®è¯´æ˜ä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­

## å›ç­”è§„èŒƒ
- å¯¹ç—‡çŠ¶åˆ†æï¼Œæä¾›å¯èƒ½çš„åŸå› å’Œå»ºè®®
- å¯¹ç§‘å®¤æŸ¥è¯¢ï¼Œç»™å‡ºæ˜ç¡®çš„ç§‘å®¤æ¨è
- å¯¹ç”¨è¯å’¨è¯¢ï¼Œè¯´æ˜ç”¨æ³•ã€æ³¨æ„äº‹é¡¹ã€å‰¯ä½œç”¨
- å¯¹å¥åº·é—®é¢˜ï¼Œæä¾›é¢„é˜²å»ºè®®å’Œç”Ÿæ´»æŒ‡å¯¼
- å¯¹é¢„çº¦è¯·æ±‚ï¼Œå¼•å¯¼ç”¨æˆ·æä¾›å¿…è¦ä¿¡æ¯

## å…è´£å£°æ˜
æ¯æ¬¡æ¶‰åŠåŒ»ç–—å»ºè®®æ—¶ï¼Œéƒ½è¦åŠ ä¸Šï¼š"ä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­å’Œæ²»ç–—ã€‚å¦‚æœ‰ä¸é€‚è¯·åŠæ—¶å°±åŒ»ã€‚"

## æ ¼å¼è¦æ±‚
- ä½¿ç”¨Markdownæ ¼å¼
- ç”¨emojiå¢åŠ å¯è¯»æ€§ (ğŸ©ºç—‡çŠ¶ã€ğŸ¥ç§‘å®¤ã€ğŸ’Šè¯å“ã€ğŸ“…é¢„çº¦ã€ğŸ“šå¥åº·)
- é‡è¦ä¿¡æ¯ç”¨åŠ ç²—æˆ–å¼•ç”¨å—çªå‡º
- æ¡ç†æ¸…æ™°ï¼Œåˆ†ç‚¹è¯´æ˜"""

    def __init__(
        self,
        api_key: str,
        base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
        model: str = "qwen-plus"
    ):
        self.llm = DashScopeLLM(api_key=api_key, base_url=base_url, model=model)
        self.conversation_history: Dict[str, List[Dict]] = {}

    async def start(self):
        """å¯åŠ¨æœåŠ¡"""
        await self.llm.start()

    async def stop(self):
        """åœæ­¢æœåŠ¡"""
        await self.llm.stop()

    def get_history(self, session_id: str) -> List[Dict]:
        """è·å–å¯¹è¯å†å²"""
        return self.conversation_history.get(session_id, [])

    def add_to_history(self, session_id: str, role: str, content: str):
        """æ·»åŠ åˆ°å¯¹è¯å†å²"""
        if session_id not in self.conversation_history:
            self.conversation_history[session_id] = []

        self.conversation_history[session_id].append({
            "role": role,
            "content": content
        })

        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…ï¼ˆæœ€è¿‘10è½®ï¼‰
        if len(self.conversation_history[session_id]) > 20:
            self.conversation_history[session_id] = self.conversation_history[session_id][-20:]

    def clear_history(self, session_id: str):
        """æ¸…é™¤å¯¹è¯å†å²"""
        if session_id in self.conversation_history:
            del self.conversation_history[session_id]

    async def generate_response(
        self,
        user_message: str,
        intent: str,
        session_id: str = "default",
        custom_prompt: str = None
    ) -> str:
        """
        ç”ŸæˆåŒ»ç–—å“åº”

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            intent: æ„å›¾ç±»å‹
            session_id: ä¼šè¯ID
            custom_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯

        Returns:
            str: ç”Ÿæˆçš„å“åº”
        """
        # æ ¹æ®æ„å›¾ç±»å‹å®šåˆ¶ç³»ç»Ÿæç¤º
        intent_prompts = {
            "symptom_inquiry": """ä½ æ˜¯ç—‡çŠ¶åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·æè¿°çš„ç—‡çŠ¶ï¼Œæä¾›ï¼š
1. å¯èƒ½çš„åŸå› åˆ†æ
2. éœ€è¦æ³¨æ„çš„å±é™©ä¿¡å·ï¼ˆçº¢æ——å¾ï¼‰
3. å»ºè®®çš„å°±è¯Šç§‘å®¤
4. è‡ªæˆ‘æŠ¤ç†å»ºè®®
5. ä½•æ—¶éœ€è¦ç«‹å³å°±åŒ»""",
            "department_query": """ä½ æ˜¯ç§‘å®¤æ¨èä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·ç—‡çŠ¶ï¼Œæ¨èæœ€åˆé€‚çš„å°±è¯Šç§‘å®¤ï¼š
1. é¦–é€‰ç§‘å®¤åŠç†ç”±
2. å¤‡é€‰ç§‘å®¤ï¼ˆå¦‚æœ‰ï¼‰
3. è¯¥ç§‘å®¤çš„è¯Šç–—èŒƒå›´""",
            "medication_consult": """ä½ æ˜¯ç”¨è¯å’¨è¯¢ä¸“å®¶ã€‚æä¾›è¯å“ç›¸å…³ä¿¡æ¯ï¼š
1. è¯å“ç”¨é€”å’Œé€‚åº”ç—‡
2. æ­£ç¡®ç”¨æ³•ç”¨é‡
3. å¸¸è§å‰¯ä½œç”¨
4. é‡è¦æ³¨æ„äº‹é¡¹å’Œç¦å¿Œ
5. ç”¨è¯æé†’""",
            "appointment": """ä½ æ˜¯é¢„çº¦æŒ‚å·åŠ©æ‰‹ã€‚å¼•å¯¼ç”¨æˆ·å®Œæˆé¢„çº¦ï¼š
1. ç¡®è®¤ç”¨æˆ·éœ€æ±‚
2. è¯¢é—®ç¼ºå¤±ä¿¡æ¯ï¼ˆç§‘å®¤ã€æ—¶é—´ã€åŒ»ç”Ÿç±»å‹ï¼‰
3. è¯´æ˜é¢„çº¦æµç¨‹
4. æä¾›æ¸©é¦¨æç¤º""",
            "health_education": """ä½ æ˜¯å¥åº·æ•™è‚²ä¸“å®¶ã€‚æä¾›ä¸“ä¸šçš„å¥åº·æŒ‡å¯¼ï¼š
1. ç–¾ç—…é¢„é˜²çŸ¥è¯†
2. å¥åº·ç”Ÿæ´»æ–¹å¼å»ºè®®
3. é¥®é£Ÿè¿åŠ¨æŒ‡å¯¼
4. é•¿æœŸç®¡ç†å»ºè®®"""
        }

        system_prompt = custom_prompt or intent_prompts.get(intent, self.SYSTEM_PROMPT)

        # è·å–å¯¹è¯å†å²
        history = self.get_history(session_id)

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        self.add_to_history(session_id, "user", user_message)

        try:
            # è°ƒç”¨LLMç”Ÿæˆå“åº”
            response = await self.llm.chat_with_system(
                user_message=user_message,
                system_prompt=system_prompt,
                conversation_history=history if len(history) <= 10 else history[-10:],
                temperature=0.7
            )

            # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°å†å²
            self.add_to_history(session_id, "assistant", response)

            return response

        except Exception as e:
            logger.error(f"[LLM] Generate response failed: {e}")
            # è¿”å›å…œåº•å“åº”
            return self._get_fallback_response(intent, user_message)

    async def generate_response_stream(
        self,
        user_message: str,
        intent: str,
        session_id: str = "default",
        custom_prompt: str = None
    ):
        """
        æµå¼ç”ŸæˆåŒ»ç–—å“åº”

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            intent: æ„å›¾ç±»å‹
            session_id: ä¼šè¯ID
            custom_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯

        Yields:
            dict: åŒ…å«typeå’Œdataçš„æµå¼äº‹ä»¶
                - {"type": "thinking", "content": "..."}
                - {"type": "content", "content": "..."}
                - {"type": "done", "content": ""}
        """
        # æ ¹æ®æ„å›¾ç±»å‹å®šåˆ¶ç³»ç»Ÿæç¤º
        intent_prompts = {
            "symptom_inquiry": """ä½ æ˜¯ç—‡çŠ¶åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·æè¿°çš„ç—‡çŠ¶ï¼Œæä¾›ï¼š
1. å¯èƒ½çš„åŸå› åˆ†æ
2. éœ€è¦æ³¨æ„çš„å±é™©ä¿¡å·ï¼ˆçº¢æ——å¾ï¼‰
3. å»ºè®®çš„å°±è¯Šç§‘å®¤
4. è‡ªæˆ‘æŠ¤ç†å»ºè®®
5. ä½•æ—¶éœ€è¦ç«‹å³å°±åŒ»""",
            "department_query": """ä½ æ˜¯ç§‘å®¤æ¨èä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·ç—‡çŠ¶ï¼Œæ¨èæœ€åˆé€‚çš„å°±è¯Šç§‘å®¤ï¼š
1. é¦–é€‰ç§‘å®¤åŠç†ç”±
2. å¤‡é€‰ç§‘å®¤ï¼ˆå¦‚æœ‰ï¼‰
3. è¯¥ç§‘å®¤çš„è¯Šç–—èŒƒå›´""",
            "medication_consult": """ä½ æ˜¯ç”¨è¯å’¨è¯¢ä¸“å®¶ã€‚æä¾›è¯å“ç›¸å…³ä¿¡æ¯ï¼š
1. è¯å“ç”¨é€”å’Œé€‚åº”ç—‡
2. æ­£ç¡®ç”¨æ³•ç”¨é‡
3. å¸¸è§å‰¯ä½œç”¨
4. é‡è¦æ³¨æ„äº‹é¡¹å’Œç¦å¿Œ
5. ç”¨è¯æé†’""",
            "appointment": """ä½ æ˜¯é¢„çº¦æŒ‚å·åŠ©æ‰‹ã€‚å¼•å¯¼ç”¨æˆ·å®Œæˆé¢„çº¦ï¼š
1. ç¡®è®¤ç”¨æˆ·éœ€æ±‚
2. è¯¢é—®ç¼ºå¤±ä¿¡æ¯ï¼ˆç§‘å®¤ã€æ—¶é—´ã€åŒ»ç”Ÿç±»å‹ï¼‰
3. è¯´æ˜é¢„çº¦æµç¨‹
4. æä¾›æ¸©é¦¨æç¤º""",
            "health_education": """ä½ æ˜¯å¥åº·æ•™è‚²ä¸“å®¶ã€‚æä¾›ä¸“ä¸šçš„å¥åº·æŒ‡å¯¼ï¼š
1. ç–¾ç—…é¢„é˜²çŸ¥è¯†
2. å¥åº·ç”Ÿæ´»æ–¹å¼å»ºè®®
3. é¥®é£Ÿè¿åŠ¨æŒ‡å¯¼
4. é•¿æœŸç®¡ç†å»ºè®®"""
        }

        system_prompt = custom_prompt or intent_prompts.get(intent, self.SYSTEM_PROMPT)

        # è·å–å¯¹è¯å†å²
        history = self.get_history(session_id)

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        self.add_to_history(session_id, "user", user_message)

        # å‘é€æ€è€ƒè¿‡ç¨‹äº‹ä»¶
        yield {
            "type": "thinking",
            "content": f"æ­£åœ¨åˆ†ææ‚¨çš„è¯·æ±‚..."
        }

        yield {
            "type": "thinking",
            "content": f"è¯†åˆ«åˆ°æ„å›¾: {intent}"
        }

        yield {
            "type": "thinking",
            "content": f"è°ƒç”¨qwen-pluså¤§æ¨¡å‹ç”Ÿæˆå›å¤..."
        }

        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [{"role": "system", "content": system_prompt}]
            if history and len(history) <= 10:
                messages.extend(history)
            messages.append({"role": "user", "content": user_message})

            # æµå¼è°ƒç”¨LLM
            full_response = ""
            async for chunk in self.llm.chat(messages, stream=True):
                full_response += chunk
                yield {
                    "type": "content",
                    "content": chunk
                }

            # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°å†å²
            self.add_to_history(session_id, "assistant", full_response)

            # å‘é€å®Œæˆäº‹ä»¶
            yield {"type": "done", "content": ""}

        except Exception as e:
            logger.error(f"[LLM] Stream generation failed: {e}")
            yield {
                "type": "error",
                "content": str(e)
            }

    def _get_fallback_response(self, intent: str, user_message: str) -> str:
        """è·å–å…œåº•å“åº”"""
        fallbacks = {
            "symptom_inquiry": f"""## å…³äºæ‚¨çš„ç—‡çŠ¶

æ„Ÿè°¢æ‚¨æè¿°çš„ç—‡çŠ¶ã€Œ{user_message}ã€ã€‚

ä¸ºäº†ç»™æ‚¨æ›´å‡†ç¡®çš„å»ºè®®ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼š
- ç—‡çŠ¶æŒç»­å¤šé•¿æ—¶é—´äº†ï¼Ÿ
- æœ‰æ²¡æœ‰å…¶ä»–ä¼´éšç—‡çŠ¶ï¼Ÿ
- ç—‡çŠ¶çš„ä¸¥é‡ç¨‹åº¦å¦‚ä½•ï¼Ÿ

> âš ï¸ **å…è´£å£°æ˜**: ä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­å’Œæ²»ç–—ã€‚å¦‚æœ‰ä¸é€‚è¯·åŠæ—¶å°±åŒ»ã€‚""",
            "department_query": f"""## ç§‘å®¤æ¨è

æ ¹æ®æ‚¨æåˆ°çš„ã€Œ{user_message}ã€ï¼Œå»ºè®®æ‚¨æŒ‚å·å‰å…ˆæ˜ç¡®å…·ä½“ç—‡çŠ¶ã€‚

å¸¸è§ç§‘å®¤å‚è€ƒï¼š
- å¤´ç—›å¤´æ™• â†’ ç¥ç»å†…ç§‘
- å’³å—½å‘çƒ­ â†’ å‘¼å¸å†…ç§‘/å‘çƒ­é—¨è¯Š
- è…¹ç—›æ¶å¿ƒ â†’ æ¶ˆåŒ–å†…ç§‘
- å¿ƒæ‚¸èƒ¸ç—› â†’ å¿ƒè¡€ç®¡å†…ç§‘

> âš ï¸ **å…è´£å£°æ˜**: ä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­å’Œæ²»ç–—ã€‚å¦‚æœ‰ä¸é€‚è¯·åŠæ—¶å°±åŒ»ã€‚""",
            "medication_consult": """## ç”¨è¯å’¨è¯¢

å…³äºè¯å“ä½¿ç”¨ï¼Œè¯·å’¨è¯¢ï¼š
1. æŸ¥é˜…è¯å“è¯´æ˜ä¹¦
2. å’¨è¯¢åŒ»é™¢è¯å¸ˆ
3. å’¨è¯¢å¼€è¯åŒ»ç”Ÿ

> âš ï¸ **é‡è¦æé†’**: è¯·ä¸¥æ ¼æŒ‰åŒ»å˜±æˆ–è¯´æ˜ä¹¦ä½¿ç”¨è¯å“ï¼Œä¸è¦è‡ªè¡Œè°ƒæ•´å‰‚é‡ã€‚""",
            "appointment": """## é¢„çº¦æŒ‚å·

è¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
1. æŒ‚å·ç§‘å®¤
2. å°±è¯Šæ—¶é—´
3. åŒ»ç”Ÿç±»å‹ï¼ˆä¸“å®¶/æ™®é€šï¼‰

> ğŸ’¡ **æç¤º**: å¦‚æœä¸ç¡®å®šæŒ‚ä»€ä¹ˆç§‘ï¼Œå¯ä»¥å…ˆå‘Šè¯‰æˆ‘æ‚¨çš„ç—‡çŠ¶ã€‚""",
            "health_education": """## å¥åº·çŸ¥è¯†

ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼ï¼š
- å‡è¡¡é¥®é£Ÿï¼Œå°‘ç›å°‘æ²¹
- é€‚é‡è¿åŠ¨ï¼Œæ¯å‘¨150åˆ†é’Ÿ
- å……è¶³ç¡çœ ï¼Œè§„å¾‹ä½œæ¯
- æˆ’çƒŸé™é…’ï¼Œä¿æŒå¥½å¿ƒæƒ…

> âš ï¸ **å…è´£å£°æ˜**: ä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­å’Œæ²»ç–—ã€‚"""
        }

        return fallbacks.get(intent, "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ï¼Œè¯·ç¨åé‡è¯•ã€‚")


# ============================================================
# å•ä¾‹ç®¡ç†
# ============================================================

_llm_service: Optional[MedicalLLMService] = None


def get_llm_service() -> Optional[MedicalLLMService]:
    """è·å–LLMæœåŠ¡å•ä¾‹"""
    return _llm_service


async def init_llm_service(api_key: str, base_url: str = None, model: str = "qwen-plus"):
    """åˆå§‹åŒ–LLMæœåŠ¡"""
    global _llm_service

    if _llm_service is None:
        _llm_service = MedicalLLMService(
            api_key=api_key,
            base_url=base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1",
            model=model
        )
        await _llm_service.start()
        logger.info("[LLM] Service initialized")

    return _llm_service


async def shutdown_llm_service():
    """å…³é—­LLMæœåŠ¡"""
    global _llm_service

    if _llm_service:
        await _llm_service.stop()
        _llm_service = None
        logger.info("[LLM] Service shutdown")
